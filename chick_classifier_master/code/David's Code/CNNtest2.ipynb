{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Import required libaries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "\n",
    "#KERAS\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# SKLEARN\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Directory location for image data - USE OWN DIRECTORY HERE\n",
    "DATADIR = '/Users/David/Desktop/CNN-image-classifier/training_data'\n",
    "#Sub directories for different categories\n",
    "CATEGORIES = [\"10_1\",\"10_3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add all images to training data\n",
    "def create_training_data():\n",
    "    for category in CATEGORIES:  # 10_1, 10_2, 10_3\n",
    "\n",
    "        path = os.path.join(DATADIR,category)  # create path to different stages\n",
    "        class_num = CATEGORIES.index(category)  # get the classification  (0, 1 or 2)\n",
    "\n",
    "        for img in tqdm(os.listdir(path)):  # iterate over each image\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img) ,cv2.IMREAD_GRAYSCALE)  # convert to array\n",
    "                training_data.append([img_array, class_num])  # add this to our training_data\n",
    "            except Exception as e:  # in the interest in keeping the output clean...\n",
    "                pass\n",
    "            #except OSError as e:\n",
    "            #    print(\"OSErrroBad img most likely\", e, os.path.join(path,img))\n",
    "            #except Exception as e:\n",
    "            #    print(\"general exception\", e, os.path.join(path,img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1296/1296 [00:01<00:00, 971.49it/s] \n",
      "100%|██████████| 1008/1008 [00:00<00:00, 1075.80it/s]\n"
     ]
    }
   ],
   "source": [
    "create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2304\n"
     ]
    }
   ],
   "source": [
    "#check all images have been added to training data\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#shuffle training_data so that 10_1s and 10_3s are not together\n",
    "import random\n",
    "\n",
    "random.shuffle(training_data)\n",
    "\n",
    "for sample in training_data[:10]:\n",
    "    print(sample[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] #Images\n",
    "y = [] #Labels\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 200, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, label in training_data:\n",
    "    X.append(features)\n",
    "    y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X).reshape(-1, img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split X and y into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols,1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols,1)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (1843, 200, 200, 1)\n",
      "1843 train samples\n",
      "461 test samples\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch_size to train\n",
    "batch_size = 32\n",
    "# number of output classes\n",
    "nb_classes = 2\n",
    "# number of epochs to train\n",
    "nb_epoch = 20\n",
    "\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# size of pooling area for max pooling\n",
    "nb_pool = 2\n",
    "# convolution kernel size\n",
    "nb_conv = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/David/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(200, 200,..., padding=\"valid\")`\n",
      "  \"\"\"\n",
      "/Users/David/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3))`\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv,\n",
    "                        border_mode='valid',\n",
    "                        input_shape=(img_rows, img_cols,1)))\n",
    "convout1 = Activation('relu')\n",
    "model.add(convout1)\n",
    "model.add(Convolution2D(nb_filters, nb_conv, nb_conv))\n",
    "convout2 = Activation('relu')\n",
    "model.add(convout2)\n",
    "model.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adadelta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/David/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/David/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1843 samples, validate on 461 samples\n",
      "Epoch 1/20\n",
      "1843/1843 [==============================] - 377s 205ms/step - loss: 0.8250 - accuracy: 0.6381 - val_loss: 0.5560 - val_accuracy: 0.7202\n",
      "Epoch 2/20\n",
      "1843/1843 [==============================] - 358s 194ms/step - loss: 0.5841 - accuracy: 0.7141 - val_loss: 0.5679 - val_accuracy: 0.7289\n",
      "Epoch 3/20\n",
      "1843/1843 [==============================] - 346s 188ms/step - loss: 0.5665 - accuracy: 0.7336 - val_loss: 0.5546 - val_accuracy: 0.7289\n",
      "Epoch 4/20\n",
      "1843/1843 [==============================] - 365s 198ms/step - loss: 0.5468 - accuracy: 0.7374 - val_loss: 0.5349 - val_accuracy: 0.7332\n",
      "Epoch 5/20\n",
      "1843/1843 [==============================] - 426s 231ms/step - loss: 0.5241 - accuracy: 0.7553 - val_loss: 0.5568 - val_accuracy: 0.7245\n",
      "Epoch 6/20\n",
      "1843/1843 [==============================] - 327s 177ms/step - loss: 0.4944 - accuracy: 0.7640 - val_loss: 0.5316 - val_accuracy: 0.7289\n",
      "Epoch 7/20\n",
      "1843/1843 [==============================] - 345s 187ms/step - loss: 0.5023 - accuracy: 0.7618 - val_loss: 0.5520 - val_accuracy: 0.7310\n",
      "Epoch 8/20\n",
      "1843/1843 [==============================] - 339s 184ms/step - loss: 0.4831 - accuracy: 0.7792 - val_loss: 0.5446 - val_accuracy: 0.7419\n",
      "Epoch 9/20\n",
      "1843/1843 [==============================] - 324s 176ms/step - loss: 0.4708 - accuracy: 0.7802 - val_loss: 0.5529 - val_accuracy: 0.7180\n",
      "Epoch 10/20\n",
      "1843/1843 [==============================] - 358s 194ms/step - loss: 0.4439 - accuracy: 0.8068 - val_loss: 0.5475 - val_accuracy: 0.7484\n",
      "Epoch 11/20\n",
      "1843/1843 [==============================] - 338s 183ms/step - loss: 0.3942 - accuracy: 0.8199 - val_loss: 0.5352 - val_accuracy: 0.7462\n",
      "Epoch 12/20\n",
      "1843/1843 [==============================] - 317s 172ms/step - loss: 0.3466 - accuracy: 0.8497 - val_loss: 0.5728 - val_accuracy: 0.7701\n",
      "Epoch 13/20\n",
      "1843/1843 [==============================] - 341s 185ms/step - loss: 0.3204 - accuracy: 0.8703 - val_loss: 0.5312 - val_accuracy: 0.7766\n",
      "Epoch 14/20\n",
      "1843/1843 [==============================] - 320s 174ms/step - loss: 0.2569 - accuracy: 0.9029 - val_loss: 0.6024 - val_accuracy: 0.7809\n",
      "Epoch 15/20\n",
      "1843/1843 [==============================] - 312s 169ms/step - loss: 0.2304 - accuracy: 0.9175 - val_loss: 0.5574 - val_accuracy: 0.8026\n",
      "Epoch 16/20\n",
      "1843/1843 [==============================] - 320s 174ms/step - loss: 0.2131 - accuracy: 0.9137 - val_loss: 0.5258 - val_accuracy: 0.7939\n",
      "Epoch 17/20\n",
      "1843/1843 [==============================] - 336s 182ms/step - loss: 0.1652 - accuracy: 0.9387 - val_loss: 0.6044 - val_accuracy: 0.8243\n",
      "Epoch 18/20\n",
      "1843/1843 [==============================] - 360s 195ms/step - loss: 0.1463 - accuracy: 0.9495 - val_loss: 0.6136 - val_accuracy: 0.8330\n",
      "Epoch 19/20\n",
      "1843/1843 [==============================] - 344s 187ms/step - loss: 0.1559 - accuracy: 0.9425 - val_loss: 0.6098 - val_accuracy: 0.8134\n",
      "Epoch 20/20\n",
      "1843/1843 [==============================] - 326s 177ms/step - loss: 0.1508 - accuracy: 0.9425 - val_loss: 0.6360 - val_accuracy: 0.8113\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a3a329518>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                verbose=1, validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test score: 0.6359502046072043\n",
      "Test accuracy: 0.811279833316803\n"
     ]
    }
   ],
   "source": [
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
